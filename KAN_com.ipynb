{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6742b5a-11ff-49a9-b79e-d7ff3232d74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/OE/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/anaconda3/envs/OE/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /opt/anaconda3/envs/OE/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/Users/sylwesterwieczorek/VulkanSDK/macOS/lib/libjpeg.9.dylib' (no such file), '/libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/OE/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/OE/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/OE/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/anaconda3/envs/OE/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cnn', 'kan_linear', 'kan_bezier', 'kan_catmull', 'kan_bspline', 'fastkan']\n",
      "[mnist][cnn] ep 001/10 | train MSE 0.04954 | val MSE 18.78598 | PSNR 16.91 dB | SSIM 0.746\n",
      "[mnist][cnn] ep 002/10 | train MSE 0.01805 | val MSE 11.16709 | PSNR 19.25 dB | SSIM 0.849\n",
      "[mnist][cnn] ep 003/10 | train MSE 0.01275 | val MSE 9.05995 | PSNR 20.08 dB | SSIM 0.876\n",
      "[mnist][cnn] ep 004/10 | train MSE 0.01044 | val MSE 7.67160 | PSNR 20.81 dB | SSIM 0.896\n",
      "[mnist][cnn] ep 005/10 | train MSE 0.00893 | val MSE 6.62932 | PSNR 21.55 dB | SSIM 0.910\n",
      "[mnist][cnn] ep 006/10 | train MSE 0.00799 | val MSE 5.96215 | PSNR 21.98 dB | SSIM 0.919\n",
      "[mnist][cnn] ep 007/10 | train MSE 0.00730 | val MSE 5.67616 | PSNR 22.14 dB | SSIM 0.919\n",
      "[mnist][cnn] ep 008/10 | train MSE 0.00677 | val MSE 5.30557 | PSNR 22.47 dB | SSIM 0.926\n",
      "[mnist][cnn] ep 009/10 | train MSE 0.00633 | val MSE 4.96060 | PSNR 22.73 dB | SSIM 0.931\n",
      "[mnist][cnn] ep 010/10 | train MSE 0.00597 | val MSE 4.72834 | PSNR 22.97 dB | SSIM 0.934\n",
      "[mnist][kan_linear] ep 001/10 | train MSE 0.02259 | val MSE 7.61438 | PSNR 20.70 dB | SSIM 0.778\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "from typing import Tuple, List, Optional, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.utils import make_grid, save_image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from skimage.metrics import peak_signal_noise_ratio, structural_similarity\n",
    "import pandas as pd\n",
    "\n",
    "FastKANModule = None\n",
    "_fastkan_import_error = None\n",
    "try:\n",
    "    # najczęściej: pip install fast-kan\n",
    "    from fastkan import FastKAN as FastKANModule\n",
    "except Exception as e1:\n",
    "    _fastkan_import_error = e1\n",
    "    try:\n",
    "        # alternatywna nazwa paczki\n",
    "        from fast_kan import FastKAN as FastKANModule\n",
    "        _fastkan_import_error = None\n",
    "    except Exception as e2:\n",
    "        _fastkan_import_error = (e1, e2)\n",
    "\n",
    "# ====================================================\n",
    "# Narzędzia\n",
    "# ====================================================\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def count_params(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "\n",
    "def to_numpy_img(batch_tensor: torch.Tensor) -> np.ndarray:\n",
    "    x = batch_tensor.detach().cpu().numpy()\n",
    "    x = np.clip(x, 0.0, 1.0)\n",
    "    return x[:, 0, :, :]\n",
    "\n",
    "\n",
    "def psnr_ssim_batch(x: torch.Tensor, y: torch.Tensor) -> Tuple[float, float]:\n",
    "    x_np = to_numpy_img(x)\n",
    "    y_np = to_numpy_img(y)\n",
    "    ps, ss = [], []\n",
    "    for i in range(x_np.shape[0]):\n",
    "        ps.append(peak_signal_noise_ratio(x_np[i], y_np[i], data_range=1.0))\n",
    "        try:\n",
    "            ss.append(structural_similarity(x_np[i], y_np[i], data_range=1.0, channel_axis=None))\n",
    "        except TypeError:\n",
    "            ss.append(structural_similarity(x_np[i], y_np[i], data_range=1.0, multichannel=False))\n",
    "    return float(np.mean(ps)), float(np.mean(ss))\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# SPLAJNY dla KAN\n",
    "# ====================================================\n",
    "\n",
    "def _linear_spline(x, grid, coeff):\n",
    "    grid_min, grid_max = grid[0], grid[-1]\n",
    "    G = grid.shape[0]\n",
    "    t = (x - grid_min) / (grid_max - grid_min) * (G - 1)\n",
    "    t = torch.clamp(t, 0, G - 1 - 1e-6)\n",
    "    t_floor = torch.floor(t)\n",
    "    frac = t - t_floor\n",
    "    li = t_floor.long().unsqueeze(-1)\n",
    "    ui = (li + 1).clamp(max=G - 1)\n",
    "    c0 = torch.gather(coeff, -1, li).squeeze(-1)\n",
    "    c1 = torch.gather(coeff, -1, ui).squeeze(-1)\n",
    "    return c0 * (1.0 - frac) + c1 * frac\n",
    "\n",
    "\n",
    "def _bezier_spline(x, grid, coeff):\n",
    "    grid_min, grid_max = grid[0], grid[-1]\n",
    "    t = torch.clamp((x - grid_min) / (grid_max - grid_min), 0.0, 1.0)\n",
    "    res = coeff\n",
    "    N = coeff.shape[-1]\n",
    "    for k in range(1, N):\n",
    "        res = res[..., :N - k] * (1 - t).unsqueeze(-1) + res[..., 1:N - k + 1] * t.unsqueeze(-1)\n",
    "    return res[..., 0]\n",
    "\n",
    "\n",
    "def _catmull_rom_spline(x, grid, coeff):\n",
    "    grid_min, grid_max = grid[0], grid[-1]\n",
    "    N = coeff.shape[-1]\n",
    "    segs = N - 3\n",
    "    t_norm = torch.clamp((x - grid_min) / (grid_max - grid_min) * segs, 0, segs - 1e-6)\n",
    "    i = torch.floor(t_norm).to(torch.int64)\n",
    "    t_loc = (t_norm - i.to(x.dtype))\n",
    "    idx = torch.stack([i, (i + 1).clamp(max=N - 1), (i + 2).clamp(max=N - 1), (i + 3).clamp(max=N - 1)], dim=-1)\n",
    "    P = torch.gather(coeff, -1, idx)\n",
    "    t = t_loc\n",
    "    t2, t3 = t * t, t * t * t\n",
    "    return 0.5 * (\n",
    "        2 * P[..., 1] +\n",
    "        (-P[..., 0] + P[..., 2]) * t +\n",
    "        (2 * P[..., 0] - 5 * P[..., 1] + 4 * P[..., 2] - P[..., 3]) * t2 +\n",
    "        (-P[..., 0] + 3 * P[..., 1] - 3 * P[..., 2] + P[..., 3]) * t3\n",
    "    )\n",
    "\n",
    "\n",
    "def _bspline_spline(x, grid, coeff):\n",
    "    grid_min, grid_max = grid[0], grid[-1]\n",
    "    N = coeff.shape[-1]\n",
    "    segs = N - 3\n",
    "    t_norm = torch.clamp((x - grid_min) / (grid_max - grid_min) * segs, 0, segs - 1e-6)\n",
    "    i = torch.floor(t_norm).to(torch.int64)\n",
    "    t_loc = t_norm - i.to(x.dtype)\n",
    "    idx = torch.stack([i, (i + 1).clamp(max=N - 1), (i + 2).clamp(max=N - 1), (i + 3).clamp(max=N - 1)], dim=-1)\n",
    "    P = torch.gather(coeff, -1, idx)\n",
    "    t = t_loc\n",
    "    t2, t3 = t * t, t * t * t\n",
    "    B0 = (1 - t) ** 3 / 6.0\n",
    "    B1 = (3 * t3 - 6 * t2 + 4) / 6.0\n",
    "    B2 = (-3 * t3 + 3 * t2 + 3 * t + 1) / 6.0\n",
    "    B3 = t3 / 6.0\n",
    "    return P[..., 0] * B0 + P[..., 1] * B1 + P[..., 2] * B2 + P[..., 3] * B3\n",
    "\n",
    "\n",
    "def spline_interpolate(x, grid, coeff, spline_type: str):\n",
    "    if spline_type == \"linear\":\n",
    "        return _linear_spline(x, grid, coeff)\n",
    "    elif spline_type == \"bezier\":\n",
    "        return _bezier_spline(x, grid, coeff)\n",
    "    elif spline_type == \"catmull_rom\":\n",
    "        return _catmull_rom_spline(x, grid, coeff)\n",
    "    elif spline_type == \"bspline\":\n",
    "        return _bspline_spline(x, grid, coeff)\n",
    "    elif spline_type in (\"hermite\", \"nurbs\"):\n",
    "        return _linear_spline(x, grid, coeff)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown spline: {spline_type}\")\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# KAN \n",
    "# ====================================================\n",
    "\n",
    "class KANLayer(nn.Module):\n",
    "    def __init__(self, n_in: int, n_out: int, num_grid: int = 6, grid_min: float = -5.0, grid_max: float = 5.0,\n",
    "                 spline_type: str = \"bspline\"):\n",
    "        super().__init__()\n",
    "        self.n_in, self.n_out, self.spline_type = n_in, n_out, spline_type\n",
    "        grid = torch.linspace(grid_min, grid_max, steps=num_grid)\n",
    "        self.register_buffer(\"grid\", grid)\n",
    "        self.coeffs = nn.Parameter(torch.randn(n_out, n_in, num_grid) * 0.05)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B = x.size(0)\n",
    "        x_e = x.unsqueeze(1).expand(B, self.n_out, self.n_in)\n",
    "        coeff_e = self.coeffs.unsqueeze(0).expand(B, -1, -1, -1)\n",
    "        sp = spline_interpolate(x_e, self.grid, coeff_e, spline_type=self.spline_type)\n",
    "        y = sp.sum(dim=-1)\n",
    "        return y\n",
    "\n",
    "\n",
    "class KANAutoencoder(nn.Module):\n",
    "    def __init__(self, H: int, W: int, latent_dim: int = 256, num_grid: int = 6, spline_type: str = \"bspline\"):\n",
    "        super().__init__()\n",
    "        self.H, self.W, self.latent_dim = H, W, latent_dim\n",
    "        self.input_dim = H * W\n",
    "        self.enc = KANLayer(self.input_dim, latent_dim, num_grid=num_grid, spline_type=spline_type)\n",
    "        self.dec = nn.Sequential(\n",
    "            KANLayer(latent_dim, self.input_dim, num_grid=num_grid, spline_type=spline_type),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B = x.size(0)\n",
    "        flat = x.view(B, -1)\n",
    "        z = self.enc(flat)\n",
    "        recon_flat = self.dec(z)\n",
    "        recon = recon_flat.view(B, 1, self.H, self.W)\n",
    "        return recon, z\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# FastKAN\n",
    "# ====================================================\n",
    "\n",
    "class FastKANAutoencoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Autoenkoder oparty o FastKAN: dwie sieci FastKAN (encoder i decoder).\n",
    "    Wymaga zainstalowanej biblioteki fast-kan / fastkan. Jeśli brak, rzuci czytelny błąd przy inicjalizacji.\n",
    "    \"\"\"\n",
    "    def __init__(self, H: int, W: int, latent_dim: int = 256):\n",
    "        super().__init__()\n",
    "        if FastKANModule is None:\n",
    "            raise ImportError(\n",
    "                \"FastKAN nie jest zainstalowany. Zainstaluj: `pip install fast-kan` (lub `pip install fastkan`). \"\n",
    "                f\"Szczegóły importu: {repr(_fastkan_import_error)}\"\n",
    "            )\n",
    "        self.H, self.W = H, W\n",
    "        input_dim = H * W\n",
    "        # Prosta architektura: warstwa wej -> latent oraz latent -> wyjście.\n",
    "        # (Biblioteka akceptuje listę rozmiarów warstw, podobnie jak w przykładzie z klasyfikacją.)\n",
    "        self.enc = FastKANModule([input_dim, latent_dim])\n",
    "        self.dec_core = FastKANModule([latent_dim, input_dim])\n",
    "        self.dec_out = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        B = x.size(0)\n",
    "        flat = x.view(B, -1)\n",
    "        z = self.enc(flat)                    # [B, L]\n",
    "        recon_flat = self.dec_out(self.dec_core(z))  # [B, H*W] -> [0,1]\n",
    "        recon = recon_flat.view(B, 1, self.H, self.W)\n",
    "        return recon, z\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# CNN\n",
    "# ====================================================\n",
    "\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self, H: int, W: int, latent_dim: int = 256, base_ch: int = 32):\n",
    "        super().__init__()\n",
    "        self.H, self.W, self.latent_dim = H, W, latent_dim\n",
    "        min_hw = min(H, W)\n",
    "        max_blocks = 4\n",
    "        target_S = 8\n",
    "        n_down = 0\n",
    "        S = min_hw\n",
    "        while S > target_S and n_down < max_blocks:\n",
    "            S = (S + 1) // 2\n",
    "            n_down += 1\n",
    "        self.S = S\n",
    "\n",
    "        # Encoder\n",
    "        enc_layers: List[nn.Module] = []\n",
    "        ch_in = 1\n",
    "        ch = base_ch\n",
    "        for _ in range(n_down):\n",
    "            enc_layers += [\n",
    "                nn.Conv2d(ch_in, ch, 3, 2, 1), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(ch, ch, 3, 1, 1), nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            ch_in = ch\n",
    "            ch = min(4 * base_ch, ch * 2)\n",
    "        self.encoder_cnn = nn.Sequential(*enc_layers) if enc_layers else nn.Identity()\n",
    "        self.encoder_gap = nn.AdaptiveAvgPool2d(1)\n",
    "        self.encoder_fc = nn.Linear(ch_in, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        dec_ch = ch_in\n",
    "        self.decoder_fc = nn.Linear(latent_dim, dec_ch * self.S * self.S)\n",
    "        dec_layers: List[nn.Module] = []\n",
    "        ch_in = dec_ch\n",
    "        for _ in range(n_down):\n",
    "            out_c = ch_in // 2 if ch_in > base_ch else base_ch\n",
    "            dec_layers += [\n",
    "                nn.ConvTranspose2d(ch_in, out_c, 3, 2, 1, output_padding=1), nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(out_c, out_c, 3, 1, 1), nn.ReLU(inplace=True),\n",
    "            ]\n",
    "            ch_in = out_c\n",
    "        self.decoder_cnn = nn.Sequential(*dec_layers) if dec_layers else nn.Identity()\n",
    "        self.decoder_out = nn.Sequential(nn.Conv2d(ch_in, 1, 3, 1, 1), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        h = self.encoder_cnn(x)\n",
    "        gap = self.encoder_gap(h).flatten(1)\n",
    "        z = self.encoder_fc(gap)\n",
    "        d = self.decoder_fc(z).view(z.size(0), -1, self.S, self.S)\n",
    "        d = self.decoder_cnn(d)\n",
    "        recon = self.decoder_out(d)\n",
    "        if recon.size(2) != self.H or recon.size(3) != self.W:\n",
    "            recon = F.interpolate(recon, size=(self.H, self.W), mode=\"bilinear\", align_corners=False)\n",
    "        return recon, z\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Zbiory danych (grayscale) + wrapper AEFromTorchvision\n",
    "# ====================================================\n",
    "\n",
    "class AEFromTorchvision(Dataset):\n",
    "    \"\"\"Owija dataset torchvision (x, label) -> (x, x) dla autoenkodera.\"\"\"\n",
    "    def __init__(self, base_ds: Dataset):\n",
    "        super().__init__()\n",
    "        self.base = base_ds\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.base[idx]\n",
    "        if isinstance(item, tuple) and len(item) >= 1:\n",
    "            x = item[0]\n",
    "        else:\n",
    "            x = item\n",
    "        return x, x\n",
    "\n",
    "\n",
    "class ShapesDataset(Dataset):\n",
    "    def __init__(self, n_samples: int = 1000, H: int = 64, W: int = 64, seed: int = 123):\n",
    "        super().__init__()\n",
    "        self.n = n_samples\n",
    "        self.H, self.W = H, W\n",
    "        rng = np.random.RandomState(seed)\n",
    "        self.params = []\n",
    "        for _ in range(n_samples):\n",
    "            shape = rng.choice([\"circle\", \"square\", \"triangle\", \"line\"])\n",
    "            cx, cy = rng.randint(8, W - 8), rng.randint(8, H - 8)\n",
    "            size = rng.randint(6, max(7, min(H, W) // 3))\n",
    "            angle = float(rng.rand() * 360.0)\n",
    "            self.params.append((shape, cx, cy, size, angle))\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    def __getitem__(self, idx):\n",
    "        import PIL.Image as Image\n",
    "        import PIL.ImageDraw as ImageDraw\n",
    "        shape, cx, cy, size, angle = self.params[idx]\n",
    "        img = Image.new(\"L\", (self.W, self.H), color=0)\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        if shape == \"circle\":\n",
    "            bbox = (cx - size, cy - size, cx + size, cy + size)\n",
    "            draw.ellipse(bbox, fill=255, outline=255)\n",
    "        elif shape == \"square\":\n",
    "            bbox = (cx - size, cy - size, cx + size, cy + size)\n",
    "            draw.rectangle(bbox, fill=255, outline=255)\n",
    "        elif shape == \"triangle\":\n",
    "            p1 = (cx, cy - size)\n",
    "            p2 = (cx - size, cy + size)\n",
    "            p3 = (cx + size, cy + size)\n",
    "            ang = math.radians(angle)\n",
    "            def rot(p):\n",
    "                x, y = p\n",
    "                xr = cx + (x - cx) * math.cos(ang) - (y - cy) * math.sin(ang)\n",
    "                yr = cy + (x - cx) * math.sin(ang) + (y - cy) * math.cos(ang)\n",
    "                return (xr, yr)\n",
    "            poly = list(map(rot, [p1, p2, p3]))\n",
    "            draw.polygon(poly, fill=255, outline=255)\n",
    "        elif shape == \"line\":\n",
    "            length = size * 2\n",
    "            x1, y1 = cx - length // 2, cy\n",
    "            x2, y2 = cx + length // 2, cy\n",
    "            ang = math.radians(angle)\n",
    "            def rot2(x, y):\n",
    "                xr = cx + (x - cx) * math.cos(ang) - (y - cy) * math.sin(ang)\n",
    "                yr = cy + (x - cx) * math.sin(ang) + (y - cy) * math.cos(ang)\n",
    "                return (xr, yr)\n",
    "            p1 = rot2(x1, y1); p2 = rot2(x2, y2)\n",
    "            draw.line([p1, p2], fill=255, width=2)\n",
    "        x = transforms.ToTensor()(img)\n",
    "        return x, x\n",
    "\n",
    "def plot_models_comparison_matplotlib(orig5: torch.Tensor,\n",
    "                                      recons_by_model: Dict[str, torch.Tensor],\n",
    "                                      order: List[str],\n",
    "                                      title: str,\n",
    "                                      out_png: str,\n",
    "                                      show: bool = True,\n",
    "                                      dpi: int = 120) -> None:\n",
    "    \"\"\"\n",
    "    Rysuje planszę porównawczą w Matplotlib z podpisami:\n",
    "    wiersz 0: 'original', kolejne: nazwy modeli z 'order' (jeśli dostępne).\n",
    "    Zapisuje do out_png i (opcjonalnie) pokazuje w Jupyterze.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_png), exist_ok=True)\n",
    "    # realna liczba wierszy (oryginał + te modele, które mamy)\n",
    "    available = [m for m in order if m in recons_by_model]\n",
    "    n_rows = 1 + len(available)\n",
    "    n_cols = min(5, orig5.size(0))\n",
    "\n",
    "    fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                             figsize=(n_cols * 2.4, n_rows * 2.4),\n",
    "                             squeeze=False)\n",
    "\n",
    "    # wiersz 0: oryginały\n",
    "    for c in range(n_cols):\n",
    "        img = orig5[c].detach().cpu().squeeze(0).numpy()\n",
    "        ax = axes[0, c]\n",
    "        ax.imshow(img, cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        if c == 0:\n",
    "            ax.set_ylabel(\"original\", fontsize=10)\n",
    "        ax.set_title(f\"img {c+1}\", fontsize=9)\n",
    "\n",
    "    # kolejne wiersze: modele\n",
    "    for r, name in enumerate(available, start=1):\n",
    "        recs = recons_by_model[name]\n",
    "        for c in range(n_cols):\n",
    "            img = recs[c].detach().cpu().squeeze(0).numpy()\n",
    "            ax = axes[r, c]\n",
    "            ax.imshow(img, cmap=\"gray\", vmin=0.0, vmax=1.0)\n",
    "            ax.set_xticks([]); ax.set_yticks([])\n",
    "            if c == 0:\n",
    "                ax.set_ylabel(name, fontsize=10)\n",
    "\n",
    "    if title:\n",
    "        fig.suptitle(title, fontsize=12, y=0.995)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(out_png, bbox_inches=\"tight\", dpi=dpi)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "def get_loaders(dataset_name: str,\n",
    "                data_root: str,\n",
    "                batch_small: int = 64,\n",
    "                batch_large: int = 16,\n",
    "                large_resize_to: int = 64,\n",
    "                custom_large_dir: Optional[str] = None) -> Tuple[DataLoader, DataLoader, Tuple[int, int]]:\n",
    "\n",
    "    if dataset_name == \"mnist\":\n",
    "        H, W = 28, 28\n",
    "        tfm = transforms.ToTensor()\n",
    "        \n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        full_train = datasets.MNIST(root=data_root, train=True, download=True, transform=tfm)\n",
    "        \n",
    "        val_size = int(0.2 * len(full_train))\n",
    "        train_size = len(full_train) - val_size\n",
    "        base_train, base_test = random_split(full_train, [train_size, val_size], generator=g)\n",
    "\n",
    "        train = AEFromTorchvision(base_train)\n",
    "        test  = AEFromTorchvision(base_test)\n",
    "        return (DataLoader(train, batch_size=batch_small, shuffle=True, num_workers=0),\n",
    "                DataLoader(test,  batch_size=batch_small, shuffle=False, num_workers=0),\n",
    "                (H, W))\n",
    "\n",
    "    elif dataset_name == \"cifar16\":\n",
    "        H, W = 16, 16\n",
    "        tfm = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((H, W), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "\n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        full_train = datasets.CIFAR10(root=data_root, train=True, download=True, transform=tfm)\n",
    "\n",
    "        val_size = int(0.2 * len(full_train))\n",
    "        train_size = len(full_train) - val_size\n",
    "        base_train, base_test = random_split(full_train, [train_size, val_size], generator=g)\n",
    "\n",
    "        train = AEFromTorchvision(base_train)\n",
    "        test  = AEFromTorchvision(base_test)\n",
    "        return (DataLoader(train, batch_size=batch_small, shuffle=True, num_workers=0),\n",
    "                DataLoader(test,  batch_size=batch_small, shuffle=False, num_workers=0),\n",
    "                (H, W))\n",
    "\n",
    "    elif dataset_name == \"large\":\n",
    "        H = W = int(large_resize_to)\n",
    "        if custom_large_dir and os.path.isdir(custom_large_dir):\n",
    "            class FolderGray(Dataset):\n",
    "                def __init__(self, root_dir: str, split: str = \"train\", val_ratio: float = 0.2):\n",
    "                    files = []\n",
    "                    for dirpath, _, filenames in os.walk(root_dir):\n",
    "                        for fn in filenames:\n",
    "                            if fn.lower().endswith((\".png\", \".jpg\", \".jpeg\", \".bmp\", \".webp\", \".tif\", \".tiff\")):\n",
    "                                files.append(os.path.join(dirpath, fn))\n",
    "                    files.sort()\n",
    "                    n = len(files)\n",
    "                    n_val = max(1, int(n * val_ratio))\n",
    "                    self.files = files[:-n_val] if split == \"train\" else files[-n_val:]\n",
    "                    self.tfm = transforms.Compose([\n",
    "                        transforms.Grayscale(num_output_channels=1),\n",
    "                        transforms.Resize((H, W), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "                        transforms.ToTensor(),\n",
    "                    ])\n",
    "                def __len__(self): return len(self.files)\n",
    "                def __getitem__(self, idx):\n",
    "                    img = default_loader(self.files[idx])\n",
    "                    x = self.tfm(img)\n",
    "                    return x, x\n",
    "            base_train = FolderGray(custom_large_dir, split=\"train\")\n",
    "            base_test  = FolderGray(custom_large_dir, split=\"val\")\n",
    "        else:\n",
    "            tfm = transforms.Compose([\n",
    "                transforms.Grayscale(num_output_channels=1),\n",
    "                transforms.Resize((H, W), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "                transforms.ToTensor(),\n",
    "            ])\n",
    "            g = torch.Generator().manual_seed(42)\n",
    "            full_train = datasets.STL10(root=data_root, download=True, transform=tfm)\n",
    "    \n",
    "            val_size = int(0.2 * len(full_train))\n",
    "            train_size = len(full_train) - val_size\n",
    "            base_train, base_test = random_split(full_train, [train_size, val_size], generator=g)\n",
    "        train = AEFromTorchvision(base_train)\n",
    "        test  = AEFromTorchvision(base_test)\n",
    "        return (DataLoader(train, batch_size=batch_large, shuffle=True, num_workers=0),\n",
    "                DataLoader(test,  batch_size=batch_large, shuffle=False, num_workers=0),\n",
    "                (H, W))\n",
    "\n",
    "    elif dataset_name == \"shapes\":\n",
    "        H, W = 32, 32\n",
    "        train = ShapesDataset(n_samples=800, H=H, W=W, seed=42)\n",
    "        test  = ShapesDataset(n_samples=200, H=H, W=W, seed=42)\n",
    "        return (DataLoader(train, batch_size=batch_small, shuffle=True, num_workers=0),\n",
    "                DataLoader(test,  batch_size=batch_small, shuffle=False, num_workers=0),\n",
    "                (H, W))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Nieznany zbiór: {dataset_name}\")\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Trening / Ewaluacja\n",
    "# ====================================================\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: torch.device, max_batches: Optional[int] = None):\n",
    "    model.eval()\n",
    "    total_loss, n_seen = 0.0, 0\n",
    "    psnrs, ssims = [], []\n",
    "    for b_idx, (x, y) in enumerate(loader):\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        recon, _ = model(x)\n",
    "        total_loss += F.mse_loss(recon, y, reduction=\"sum\").item()\n",
    "        n_seen += x.size(0)\n",
    "        p, s = psnr_ssim_batch(y, recon)\n",
    "        psnrs.append(p); ssims.append(s)\n",
    "        if max_batches is not None and (b_idx + 1) >= max_batches:\n",
    "            break\n",
    "    return total_loss / max(1, n_seen), float(np.mean(psnrs)), float(np.mean(ssims))\n",
    "\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader, opt: torch.optim.Optimizer, device: torch.device):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    for x, y in loader:\n",
    "        x = x.to(device); y = y.to(device)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        recon, _ = model(x)\n",
    "        loss = F.mse_loss(recon, y)\n",
    "        loss.backward(); opt.step()\n",
    "        running += loss.item() * x.size(0)\n",
    "    return running / len(loader.dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def measure_inference_time(model: nn.Module, loader: DataLoader, device: torch.device, warmup: int = 2, measure_batches: int = 5) -> float:\n",
    "    model.eval()\n",
    "    it = iter(loader)\n",
    "    for _ in range(warmup):\n",
    "        try:\n",
    "            x, _ = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader); x, _ = next(it)\n",
    "        x = x.to(device); _ = model(x)\n",
    "    times = []\n",
    "    it = iter(loader)\n",
    "    for _ in range(measure_batches):\n",
    "        try:\n",
    "            x, _ = next(it)\n",
    "        except StopIteration:\n",
    "            it = iter(loader); x, _ = next(it)\n",
    "        x = x.to(device)\n",
    "        t0 = time.perf_counter(); _ = model(x); t1 = time.perf_counter()\n",
    "        times.append((t1 - t0) / x.size(0))\n",
    "    return float(np.mean(times))\n",
    "\n",
    "\n",
    "# ====================================================\n",
    "# Budowa modeli + zapis porównań obrazków\n",
    "# ====================================================\n",
    "\n",
    "def build_model(kind: str, H: int, W: int, latent_dim: int, kan_grid: int) -> nn.Module:\n",
    "    if kind == \"cnn\":\n",
    "        return CNNAutoencoder(H, W, latent_dim=latent_dim, base_ch=32)\n",
    "    elif kind.startswith(\"kan_\"):\n",
    "        spline_map = {\"kan_linear\": \"linear\", \"kan_bezier\": \"bezier\", \"kan_catmull\": \"catmull_rom\", \"kan_bspline\": \"bspline\"}\n",
    "        return KANAutoencoder(H, W, latent_dim=latent_dim, num_grid=kan_grid, spline_type=spline_map[kind])\n",
    "    elif kind == \"fastkan\":\n",
    "        return FastKANAutoencoder(H, W, latent_dim=latent_dim)\n",
    "    else:\n",
    "        raise ValueError(f\"Nieznany model: {kind}\")\n",
    "\n",
    "\n",
    "def save_sample_recons(model: nn.Module, loader: DataLoader, device: torch.device, out_path: str, n_batches: int = 1):\n",
    "    model.eval(); os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    imgs = []\n",
    "    with torch.no_grad():\n",
    "        it = iter(loader)\n",
    "        for _ in range(n_batches):\n",
    "            try: x, _ = next(it)\n",
    "            except StopIteration: break\n",
    "            x = x.to(device); recon, _ = model(x)\n",
    "            grid = make_grid(torch.cat([x[:8], recon[:8]], dim=0), nrow=8)\n",
    "            imgs.append(grid.cpu())\n",
    "    if imgs: save_image(torch.stack(imgs).mean(dim=0), out_path)\n",
    "\n",
    "\n",
    "def save_models_comparison(orig5: torch.Tensor,\n",
    "                           recons_by_model: Dict[str, torch.Tensor],\n",
    "                           order: List[str],\n",
    "                           out_png: str,\n",
    "                           legend_txt: str):\n",
    "    \"\"\"\n",
    "    Tworzy wspólną planszę: pierwszy wiersz to oryginały (5 obrazków),\n",
    "    kolejne wiersze to rekonstrukcje wg kolejności w 'order' (jeśli dostępne).\n",
    "    Zapisuje PNG + plik z legendą.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_png), exist_ok=True)\n",
    "    rows = []\n",
    "    rows.append(make_grid(orig5[:5].cpu(), nrow=5, padding=2))\n",
    "    legend_lines = [\"Row 0: original\"]\n",
    "    row_idx = 1\n",
    "    for name in order:\n",
    "        if name in recons_by_model:\n",
    "            rows.append(make_grid(recons_by_model[name][:5].cpu(), nrow=5, padding=2))\n",
    "            legend_lines.append(f\"Row {row_idx}: {name}\")\n",
    "            row_idx += 1\n",
    "    if not rows:\n",
    "        return\n",
    "    hcat = rows[0]\n",
    "    for r in rows[1:]:\n",
    "        if r.shape[2] != hcat.shape[2]:\n",
    "            # wycentruj/pad do tej samej szerokości\n",
    "            W = max(r.shape[2], hcat.shape[2])\n",
    "            def _pad_to(t, W):\n",
    "                pad = W - t.shape[2]\n",
    "                if pad <= 0: return t[:, :, :W]\n",
    "                left = pad // 2; right = pad - left\n",
    "                return F.pad(t, (left, right, 0, 0))\n",
    "            hcat = _pad_to(hcat, W)\n",
    "            r = _pad_to(r, W)\n",
    "        hcat = torch.cat([hcat, r], dim=1)  # po wysokości\n",
    "    save_image(hcat, out_png)\n",
    "    with open(legend_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(legend_lines))\n",
    "\n",
    "def run_all(datasets: List[str] = None,\n",
    "            models: List[str] = None,\n",
    "            data_root: str = \"./data\",\n",
    "            latent_dim: int = 256,\n",
    "            kan_grid: int = 6,\n",
    "            epochs_small: int = 10,\n",
    "            epochs_large: int = 8,\n",
    "            batch_small: int = 64,\n",
    "            batch_large: int = 16,\n",
    "            large_resize: int = 64,\n",
    "            large_dir: Optional[str] = None,\n",
    "            lr: float = 1e-3,\n",
    "            max_eval_batches: Optional[int] = None,\n",
    "            save_samples: bool = False,\n",
    "            outdir: str = \"./outputs\",\n",
    "            csv_path: str = \"./outputs/results_equal_latent.csv\",\n",
    "            seed: int = 42) -> pd.DataFrame:\n",
    "\n",
    "    set_seed(seed); device = torch.device(\"cpu\"); os.makedirs(outdir, exist_ok=True)\n",
    "    if datasets is None: datasets = [\"mnist\", \"cifar16\", \"large\", \"shapes\"]\n",
    "    if models is None: models = [\"cnn\", \"kan_linear\", \"kan_bezier\", \"kan_catmull\", \"kan_bspline\", \"fastkan\"]\n",
    "\n",
    "    print(models)\n",
    "\n",
    "    results: List[Dict] = []\n",
    "\n",
    "    for ds in datasets:\n",
    "        train_loader, test_loader, (H, W) = get_loaders(\n",
    "            ds, data_root,\n",
    "            batch_small=batch_small,\n",
    "            batch_large=batch_large,\n",
    "            large_resize_to=large_resize,\n",
    "            custom_large_dir=large_dir,\n",
    "        )\n",
    "        epochs = epochs_small if ds in (\"mnist\", \"cifar16\", \"shapes\") else epochs_large\n",
    "\n",
    "        # 5 obrazów do porównań między modelami\n",
    "        cmp_batch = next(iter(test_loader))[0][:5].to(device)  # [5,1,H,W]\n",
    "        recons_by_model: Dict[str, torch.Tensor] = {}\n",
    "\n",
    "        for kind in models:\n",
    "            model = build_model(kind, H, W, latent_dim=latent_dim, kan_grid=kan_grid)\n",
    "            model.to(device)\n",
    "            n_params = count_params(model)\n",
    "            opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            best_val = float(\"inf\"); best_state = None\n",
    "\n",
    "            for ep in range(1, epochs + 1):\n",
    "                tr_loss = train_one_epoch(model, train_loader, opt, device)\n",
    "                val_loss, val_psnr, val_ssim = evaluate(model, test_loader, device, max_batches=max_eval_batches)\n",
    "                if val_loss < best_val:\n",
    "                    best_val = val_loss\n",
    "                    best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "                print(f\"[{ds}][{kind}] ep {ep:03d}/{epochs} | train MSE {tr_loss:.5f} | val MSE {val_loss:.5f} | PSNR {val_psnr:.2f} dB | SSIM {val_ssim:.3f}\")\n",
    "\n",
    "            if best_state is not None: model.load_state_dict(best_state)\n",
    "            val_loss, val_psnr, val_ssim = evaluate(model, test_loader, device, max_batches=max_eval_batches)\n",
    "            sec_per_img = measure_inference_time(model, test_loader, device, warmup=2, measure_batches=5)\n",
    "\n",
    "            # Zapisywanie próbki\n",
    "            if save_samples:\n",
    "                out_png = os.path.join(outdir, f\"samples_{ds}_{kind}.png\")\n",
    "                save_sample_recons(model, test_loader, device, out_png, n_batches=1)\n",
    "\n",
    "            # dodawanie rekonstrukcje do planszy\n",
    "            with torch.no_grad():\n",
    "                recon, _ = model(cmp_batch)\n",
    "            recons_by_model[kind] = recon.detach().cpu()\n",
    "\n",
    "            results.append({\n",
    "                \"dataset\": ds,\n",
    "                \"H\": H, \"W\": W,\n",
    "                \"model\": kind,\n",
    "                \"latent_dim\": latent_dim,\n",
    "                \"params\": n_params,\n",
    "                \"epochs\": epochs,\n",
    "                \"val_mse\": val_loss,\n",
    "                \"val_psnr\": val_psnr,\n",
    "                \"val_ssim\": val_ssim,\n",
    "                \"infer_ms_per_img\": sec_per_img * 1000.0,\n",
    "            })\n",
    "            del model\n",
    "\n",
    "        # po przejściu wszystkich modeli dla danego zbioru — zapisz wspólną planszę porównawczą\n",
    "        compare_png = os.path.join(outdir, f\"compare_{ds}.png\")\n",
    "        compare_legend = os.path.join(outdir, f\"compare_{ds}.txt\")\n",
    "        save_models_comparison(cmp_batch.cpu(), recons_by_model, models, compare_png, compare_legend)\n",
    "        print(f\"== Zapisano porównanie obrazów do: {compare_png} (legenda: {compare_legend})\")\n",
    "\n",
    "        # NOWE: Matplotlib z etykietami (pokazuje w Jupyterze + zapisuje)\n",
    "        compare_mpl = os.path.join(outdir, f\"compare_{ds}_mpl.png\")\n",
    "        plot_models_comparison_matplotlib(\n",
    "            orig5=cmp_batch.cpu(),\n",
    "            recons_by_model=recons_by_model,\n",
    "            order=models,\n",
    "            title=f\"Dataset: {ds}  |  models: {', '.join([m for m in models if m in recons_by_model])}\",\n",
    "            out_png=compare_mpl,\n",
    "            show=True,        # w Jupyterze wyświetli od razu\n",
    "            dpi=120\n",
    "        )\n",
    "        print(f\"== Zapisano porównanie Matplotlib do: {compare_mpl}\")\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    os.makedirs(os.path.dirname(csv_path), exist_ok=True)\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(\"\\n== Zapisano wyniki do:\", csv_path)\n",
    "    print(df)\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_args_jupyter_friendly():\n",
    "    parser = argparse.ArgumentParser(description=\"Porównanie AE: CNN, KAN (splajny), FastKAN — wspólny latent\")\n",
    "    parser.add_argument(\"--data-root\", type=str, default=\"./data\")\n",
    "    parser.add_argument(\"--datasets\", type=str, default=\"mnist,cifar16,large,shapes\")\n",
    "    parser.add_argument(\"--models\", type=str, default=\"cnn,kan_linear,kan_bezier,kan_catmull,kan_bspline,fastkan\")\n",
    "    parser.add_argument(\"--latent-dim\", type=int, default=16)\n",
    "    parser.add_argument(\"--kan-grid\", type=int, default=6)\n",
    "    parser.add_argument(\"--epochs-small\", type=int, default=10)\n",
    "    parser.add_argument(\"--epochs-large\", type=int, default=10)\n",
    "    parser.add_argument(\"--batch-small\", type=int, default=64)\n",
    "    parser.add_argument(\"--batch-large\", type=int, default=16)\n",
    "    parser.add_argument(\"--large-resize\", type=int, default=64)\n",
    "    parser.add_argument(\"--large-dir\", type=str, default=None)\n",
    "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
    "    parser.add_argument(\"--max-eval-batches\", type=int, default=None)\n",
    "    parser.add_argument(\"--save-samples\", action=\"store_true\")\n",
    "    parser.add_argument(\"--outdir\", type=str, default=\"./outputs\")\n",
    "    parser.add_argument(\"--csv\", type=str, default=\"./outputs/results_equal_latent_16_no_silu_test.csv\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=42)\n",
    "    args, _unknown = parser.parse_known_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "latens = [256, 128, 64, 32, 16];\n",
    "\n",
    "for lat in latens:\n",
    "    run_all(\n",
    "        datasets=[\"mnist\",\"cifar16\",\"large\",\"shapes\"],\n",
    "        models=[\"cnn\",\"kan_linear\",\"kan_bezier\",\"kan_catmull\",\"kan_bspline\",\"fastkan\"],\n",
    "        data_root=\"./data\",\n",
    "        latent_dim=lat,\n",
    "        kan_grid=6,\n",
    "        epochs_small=10,\n",
    "        epochs_large=10,\n",
    "        batch_small=64,\n",
    "        batch_large=64,\n",
    "        large_resize=64,\n",
    "        large_dir=None,\n",
    "        lr=1e-3,\n",
    "        max_eval_batches=None,\n",
    "        save_samples=\"store_true\",\n",
    "        outdir=\"./outputs\",\n",
    "        csv_path=f\"./outputs/results_final_{lat}.csv\",\n",
    "        seed=42,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a833833-c884-481a-92c1-ec3895f3864d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
